(venv) $ python finetune_plm_hftrainer.py --help
usage: finetune_plm_hftrainer.py [-h] --model_fpath MODEL_FPATH --train TRAIN
                                 --valid VALID [--logs LOGS] [--ckpt CKPT]
                                 [--pretrained_model_name PRETRAINED_MODEL_NAME]
                                 [--per_replica_batch_size PER_REPLICA_BATCH_SIZE]
                                 [--n_epochs N_EPOCHS]
                                 [--warmup_ratio WARMUP_RATIO] [--lr LR]
                                 [--weight_decay WEIGHT_DECAY]
                                 [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]
                                 [--inp_max_len INP_MAX_LEN]
                                 [--tar_max_len TAR_MAX_LEN]

optional arguments:
  -h, --help            show this help message and exit
  --model_fpath MODEL_FPATH
                        The name of the trained model being saved except the
                        extension. (ex: hft.gogamza.kobart-
                        base-v1.bs-16*2*8.lr-5e-5.wd-1e-2.warmup-2.adamw)
  --train TRAIN         Training *.tsv file name including columns named [id,
                        text, summary]. (ex: ./data/train.tsv)
  --valid VALID         Validate *.tsv file name including columns named [id,
                        text, summary]. (ex: ./data/valid.tsv)
  --logs LOGS           Top-level folder where logs for Tensorboard
                        visualizations are stored. Logs are automatically
                        written inside the sub-folder 'YYYYmmDD-HHMMSS'. (ex:
                        ./logs/20211205-164445/{SOME_LOGS...} Default=logs
  --ckpt CKPT           The top-level folder path where checkpoints are
                        stored. In addition to the model automatically saved
                        by Huggingface trainer, the checkpoint with the
                        lowest(=best) validation loss will be saved with the
                        *.pth extension by adding the current time in front of
                        the model name specified in 'model_fpath' argument.
                        Default=ckpt
  --pretrained_model_name PRETRAINED_MODEL_NAME
                        Calls from models published to Huggingface Hub. See:
                        https://huggingface.co/models. Default=gogamza/kobart-
                        summarization
  --per_replica_batch_size PER_REPLICA_BATCH_SIZE
                        Batch size allocated per GPU. If only 1 GPU is
                        available, it is the same value as
                        'global_batch_size'. Default=48
  --n_epochs N_EPOCHS   The number of iterations of training & validation for
                        the entire dataset. Default=5
  --warmup_ratio WARMUP_RATIO
                        The ratio of warm-up iterations that gradulally
                        increase compared to the total number of iterations.
                        Default=0.2
  --lr LR               The learning rate. Default=5e-05
  --weight_decay WEIGHT_DECAY
                        Weight decay applied to the AdamW optimizer.
                        Default=0.01
  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS
                        Number of updates steps to accumulate the gradients
                        for, before performing a backward/update pass.
                        Default=1
  --inp_max_len INP_MAX_LEN
                        A value for slicing the input data. It is important to
                        note that the upper limit is determined by the
                        embedding value of the model you want to use.
                        Default=1024
  --tar_max_len TAR_MAX_LEN
                        A value for slicing the output data. It is used for
                        model inference. if the value is too small, the
                        summary may be truncated before completion.
                        Default=512
