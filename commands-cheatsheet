## EDA
# python eda.py \
#     --raw_train ./data/raw/Training \
#     --raw_valid ./data/raw/Validation \
#     --raw_test ./data/raw/Test

## Preprocess
python preprocess.py \
    --raw_train ./data/raw/Training \
    --raw_valid ./data/raw/Validation \
    --raw_test ./data/raw/Test

## Make dev dataset.
# head -n 1000 ./data/train > ./data/train.dev &
# head -n 1000 ./data/train > ./data/train.dev &
# head -n 1000 ./data/valid > ./data/valid.dev &
# head -n 1000 ./data/valid > ./data/valid.dev &

## Train: native
# python finetune_plm_native.py \
#     --train ./data/train.tsv \
#     --valid ./data/valid.tsv \
#     --gpu_id 0 \
#     --max_grad_norm 1e+8 \
#     --batch_size 48 \
#     --lr 5e-5 \
#     --n_epochs 10 \
#     --use_radam \
#     --model_fpath hft.gogamza.kobart-base-v1.bs-16.lr-5e-5.warmup-2.radam.

## Train: hftariner.
python finetune_plm_hftrainer.py \
    --train ./data/train.tsv \
    --valid ./data/valid.tsv \
    --pretrained_model_name gogamza/kobart-base-v1 \
    --per_replica_batch_size 16 \
    --lr 5e-5 \
    --weight_decay 1e-2 \
    --gradient_accumulation_steps 8 \
    --n_epochs 10 \
    --model_fpath hft.gogamza.kobart-base-v1.bs-16*2*8.lr-5e-5.wd-1e-2.warmup-2.adamw

## Inference.
python summarize.py \
    --test ./data/test.tsv \
    --model_fpath ./ckpt/20211205-164445/hft.gogamza.kobart-base-v1.bs-16*2*8.lr-5e-5.wd-1e-2.warmup-2.adamw.pth \
    --gpu_id 0 \
    --length_penalty 0.6 \
    --batch_size 64 \
    --no_repeat_ngram_size 3

## Auto-submission API.
wget -O dacon_submit_api-0.0.4-py3-none-any.zip https://bit.ly/3gMPScE
unzip dacon_submit_api-0.0.4-py3-none-any.zip
pip install dacon_submit_api-0.0.4-py3-none-any.whl

## Remote server terminal.
tensorboard --logdir ./logs --port 8888 --bind_all
