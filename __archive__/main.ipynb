{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4d305a",
   "metadata": {},
   "source": [
    "# **DACON-GAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467ea213",
   "metadata": {},
   "source": [
    "## **Default Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be861cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VERSION]\n",
      "torch: 1.10.0\n",
      "torch_optimizer: 0.3.0\n",
      "transformers: 4.11.3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_optimizer\n",
    "import transformers\n",
    "\n",
    "import collections\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "print(\"[VERSION]\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"torch_optimizer: {torch_optimizer.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14ef510b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'data': PosixPath('data'),\n",
      "    'tr_data': PosixPath('data/tr'),\n",
      "    'tr_law_data': PosixPath('data/tr/tr_law_data.json'),\n",
      "    'tr_journal_data': PosixPath('data/tr/tr_journal_data.json'),\n",
      "    'tr_article_data': PosixPath('data/tr/tr_article_data.json'),\n",
      "    'vl_data': PosixPath('data/vl'),\n",
      "    'vl_law_data': PosixPath('data/vl/vl_law_data.json'),\n",
      "    'vl_journal_data': PosixPath('data/vl/vl_journal_data.json'),\n",
      "    'vl_article_data': PosixPath('data/vl/vl_article_data.json'),\n",
      "    'ts_data': PosixPath('data/ts'),\n",
      "    'ts_all_data': PosixPath('data/ts/test.jsonl'),\n",
      "    'ts_sample_submission': PosixPath('data/ts/sample_submission.csv'),\n",
      "    'ckpt_dir': PosixPath('ckpt'),\n",
      "    'log_dir': PosixPath('ckpt'),\n",
      "    'seed': 42,\n",
      "    'tokenizer_name': 'beomi/KcELECTRA-base',\n",
      "    'tokenizer': PreTrainedTokenizerFast(name_or_path='beomi/KcELECTRA-base', vocab_size=50135, model_max_len=512, is_fast=True, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),\n",
      "    'pt_path': PosixPath('data'),\n",
      "    'max_cls_len': 50,\n",
      "    'per_replica_batch_size': 4,\n",
      "    'global_batch_size': 4,\n",
      "    'num_workers': 4,\n",
      "    'model_name': None,\n",
      "    'n_gpus': 1,\n",
      "    'device': device(type='cuda'),\n",
      "    'd_model': 768,\n",
      "    'nhead': 12,\n",
      "    'dropout': 0.1,\n",
      "    'batch_first': True,\n",
      "    'num_layers': 2,\n",
      "    'lr': 2e-05,\n",
      "    'weight_decay': 0.0001,\n",
      "    'pct_start': 0.1,\n",
      "    'max_lr': 2e-05,\n",
      "    'epochs': 10,\n",
      "    'monitor': 'vl_loss',\n",
      "    'max_to_keep': 3}\n"
     ]
    }
   ],
   "source": [
    "class HParams():\n",
    "    \n",
    "    def __init__(self):\n",
    "        ## Path.\n",
    "        self.data = Path(\"data\") ## 문서요약 텍스트\n",
    "        \n",
    "        self.tr_data = self.data / Path(\"tr\")\n",
    "        self.tr_law_data = self.tr_data / Path(\"tr_law_data.json\")\n",
    "        self.tr_journal_data = self.tr_data / Path(\"tr_journal_data.json\")\n",
    "        self.tr_article_data = self.tr_data / Path(\"tr_article_data.json\")\n",
    "\n",
    "        self.vl_data = self.data / Path(\"vl\")\n",
    "        self.vl_law_data = self.vl_data / Path(\"vl_law_data.json\")\n",
    "        self.vl_journal_data = self.vl_data / Path(\"vl_journal_data.json\")\n",
    "        self.vl_article_data = self.vl_data / Path(\"vl_article_data.json\")\n",
    "        \n",
    "        self.ts_data = self.data / Path(\"ts\")\n",
    "        self.ts_all_data = self.ts_data / Path(\"test.jsonl\")\n",
    "        self.ts_sample_submission = self.ts_data / Path(\"sample_submission.csv\")\n",
    "        \n",
    "        self.ckpt_dir = Path(\"ckpt\")\n",
    "        self.log_dir = Path(\"ckpt\") ## record log in ckpt path.\n",
    "\n",
    "        ## Seed.\n",
    "        self.seed = 42\n",
    "        \n",
    "        ## Dataloader\n",
    "        self.tokenizer_name = \"beomi/KcELECTRA-base\"\n",
    "        ## Avoid using `tokenizers` before the fork if possible.\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.pt_path = self.data\n",
    "        self.max_cls_len = 50 ## only 50 sentences are allowed.\n",
    "        \n",
    "        self.per_replica_batch_size = 4 ## multiple of 8\n",
    "        self.global_batch_size = self.per_replica_batch_size * torch.cuda.device_count() ## 32\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        ## Modeling.\n",
    "        self.model_name = None\n",
    "        self.n_gpus = torch.cuda.device_count()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        ## Transformer Decoder.\n",
    "        self.d_model = 768\n",
    "        self.nhead = 12\n",
    "        self.dropout = 0.1\n",
    "        self.batch_first = True\n",
    "        \n",
    "        self.num_layers = 2\n",
    "        \n",
    "        ## Optimizer.\n",
    "        self.lr = 2e-5\n",
    "        self.weight_decay = 1e-4\n",
    "        \n",
    "        ## Scheduler.\n",
    "        self.pct_start = 0.1\n",
    "        self.max_lr = self.lr\n",
    "        self.epochs = 10\n",
    "        \n",
    "        ## Checkpoint manager.\n",
    "        self.monitor = \"vl_loss\"\n",
    "        self.max_to_keep = 3\n",
    "        \n",
    "        ## Set environments.\n",
    "        self._set_os_environments()\n",
    "        self._seed_everything(self.seed)\n",
    "        \n",
    "    \n",
    "    def _set_os_environments(self):\n",
    "        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "        \n",
    "    def _seed_everything(self, seed: int):\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "        torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "        \n",
    "        \n",
    "args = HParams()\n",
    "pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99a40ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  8 11:40:03 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.74       Driver Version: 470.74       CUDA Version: 11.4     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  Off  | 00000000:0A:00.0  On |                  N/A |\n",
      "|  0%   37C    P8    16W / 220W |    468MiB /  7979MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1000      G   /usr/lib/xorg/Xorg                 35MiB |\n",
      "|    0   N/A  N/A      1918      G   /usr/lib/xorg/Xorg                130MiB |\n",
      "|    0   N/A  N/A      2054      G   /usr/bin/gnome-shell              101MiB |\n",
      "|    0   N/A  N/A      2382      G   ...AAAAAAAAA= --shared-files      157MiB |\n",
      "|    0   N/A  N/A      3256      G   ...AAAAAAAAA= --shared-files       27MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           31Gi       3.3Gi        25Gi       199Mi       2.6Gi        27Gi\n",
      "Swap:         2.0Gi          0B       2.0Gi\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi; free -h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf264323",
   "metadata": {},
   "source": [
    "## **Prepare Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad5cff6",
   "metadata": {},
   "source": [
    "### **Naming**\n",
    "\n",
    "제공받은 데이터세트의 이름을 조금 변경했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b78879e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !tree -alh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f8272",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Data Format**\n",
    "\n",
    "참조: https://aihub.or.kr/aidata/8054"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f74b27a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{   'name': '법률문서 프로젝트',\n",
      "    'delivery_date': '2020-12-23 17:23:13',\n",
      "    'documents': [   {   'id': '100004',\n",
      "                         'category': '일반행정',\n",
      "                         'size': 'small',\n",
      "                         'char_count': 377,\n",
      "                         'publish_date': '19841226',\n",
      "                         'title': '부당노동행위구제재심판정취소',\n",
      "                         'text': [   [   {   'index': 0,\n",
      "                                             'sentence': '원고가 소속회사의 노동조합에서 분규가 '\n",
      "                                                         '발생하자 노조활동을 구실로 정상적인 '\n",
      "                                                         '근무를 해태하고,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 1,\n",
      "                                             'sentence': '노조조합장이 사임한 경우,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 2,\n",
      "                                             'sentence': '노동조합규약에 동 조합장의 직무를 '\n",
      "                                                         '대행할 자를 규정해 두고 있음에도 '\n",
      "                                                         '원고 자신이 주동하여 '\n",
      "                                                         '노조자치수습대책위원회를 구성하여 그 '\n",
      "                                                         '위원장으로 피선되어 근무시간중에도 '\n",
      "                                                         '노조활동을 벌여 운수업체인 소속회사의 '\n",
      "                                                         '업무에 지장을 초래하고',\n",
      "                                             'highlight_indices': '8,9;68,69'},\n",
      "                                         {   'index': 3,\n",
      "                                             'sentence': '종업원들에게도 나쁜 영향을 끼쳐 '\n",
      "                                                         '소속회사가 취업규칙을 위반하고',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 4,\n",
      "                                             'sentence': '고의로 회사업무능률을 저해하였으며 '\n",
      "                                                         '회사업무상의 지휘명령에 위반하였음을 '\n",
      "                                                         '이유로 원고를 징계해고 하였다면,',\n",
      "                                             'highlight_indices': '0,3'},\n",
      "                                         {   'index': 5,\n",
      "                                             'sentence': '이는 원고의 노동조합 활동과는 '\n",
      "                                                         '관계없이 회사취업규칙에 의하여 '\n",
      "                                                         '사내질서를 유지하기 위한 사용자 '\n",
      "                                                         '고유의 징계권에 기하여 이루어진 '\n",
      "                                                         '정당한 징계권의 행사로 보아야 한다.',\n",
      "                                             'highlight_indices': '17,21'}]],\n",
      "                         'annotator_id': 3783,\n",
      "                         'document_quality_scores': {   'readable': 3,\n",
      "                                                        'accurate': 3,\n",
      "                                                        'informative': 3,\n",
      "                                                        'trustworthy': 3},\n",
      "                         'extractive': [5, 4, 2],\n",
      "                         'abstractive': [   '원고가  주동하여 회사업무능률을 저해하고 회사업무상의 '\n",
      "                                            '지휘명령에 위반하였다면 이에 따른 징계해고는 사내질서를 '\n",
      "                                            '유지하기 위한 사용자 고유의 정당한 징계권의 행사로 보아야 '\n",
      "                                            '한다.']},\n",
      "                     {   'id': '83586',\n",
      "                         'category': '민사',\n",
      "                         'size': 'small',\n",
      "                         'char_count': 491,\n",
      "                         'publish_date': '20041126',\n",
      "                         'title': '손해배상(자)',\n",
      "                         'text': [   [   {   'index': 0,\n",
      "                                             'sentence': '[1] 교통사고 피해자의 기왕증이 그 '\n",
      "                                                         '사고와 경합하여 악화됨으로써 '\n",
      "                                                         '피해자에게 특정 상해의 발현 또는 '\n",
      "                                                         '치료기간의 장기화,',\n",
      "                                             'highlight_indices': '19,20;53,55'},\n",
      "                                         {   'index': 1,\n",
      "                                             'sentence': '나아가 치료종결 후 후유장해 정도의 '\n",
      "                                                         '확대라는 결과 발생에 기여한 '\n",
      "                                                         '경우에는,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 2,\n",
      "                                             'sentence': '기왕증이 그 특정 상해를 포함한 상해 '\n",
      "                                                         '전체의 결과 발생에 대하여 '\n",
      "                                                         '기여하였다고 인정되는 정도에 따라 '\n",
      "                                                         '피해자의 전 손해 중 그에 상응한 '\n",
      "                                                         '배상액을 부담케 하는 것이 손해의 '\n",
      "                                                         '공평한 부담이라는 견지에서 타당하고,',\n",
      "                                             'highlight_indices': '5,6;60,61'},\n",
      "                                         {   'index': 3,\n",
      "                                             'sentence': '법원이 기왕증의 상해 전체에 대한 '\n",
      "                                                         '기여도를 정함에 있어서는 반드시 '\n",
      "                                                         '의학상으로 정확히 판정하여야 하는 '\n",
      "                                                         '것은 아니며,',\n",
      "                                             'highlight_indices': '33,36;43,46'},\n",
      "                                         {   'index': 4,\n",
      "                                             'sentence': '변론에 나타난 기왕증의 원인과 정도,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 5,\n",
      "                                             'sentence': '상해의 부위 및 정도,',\n",
      "                                             'highlight_indices': '7,8'},\n",
      "                                         {   'index': 6,\n",
      "                                             'sentence': '기왕증과 전체 상해와의 상관관계,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 7,\n",
      "                                             'sentence': '치료경과, 피해자의 연령과 직업 및 '\n",
      "                                                         '건강상태 등 제반 사정을 고려하여 '\n",
      "                                                         '합리적으로 판단할 수 있다.',\n",
      "                                             'highlight_indices': '18,19'}],\n",
      "                                     [   {   'index': 8,\n",
      "                                             'sentence': '[2] 교통사고 피해자의 기왕증 등이 '\n",
      "                                                         '손해 확대에 기여한 부분이 있음에도,',\n",
      "                                             'highlight_indices': ''},\n",
      "                                         {   'index': 9,\n",
      "                                             'sentence': '입원치료기간 중의 일실수입을 산정함에 '\n",
      "                                                         '있어 이를 참작하지 않은 원심판결을 '\n",
      "                                                         '파기한 사례.',\n",
      "                                             'highlight_indices': '43,44'}]],\n",
      "                         'annotator_id': 3684,\n",
      "                         'document_quality_scores': {   'readable': 4,\n",
      "                                                        'accurate': 4,\n",
      "                                                        'informative': 4,\n",
      "                                                        'trustworthy': 4},\n",
      "                         'extractive': [0, 2, 9],\n",
      "                         'abstractive': [   '교통사고 피해자의 기왕증이 그 사고와 합쳐져 악화됨으로써 '\n",
      "                                            '피해자에게 특정 상해의 발현 또는 치료기간의 장기화에 영향을 '\n",
      "                                            '준 때에는 기왕증이 그 특정 상해를 포함한 상해 전체의 결과 '\n",
      "                                            '발생에 대하여 영향을 주었다고 판단되는 정도에 따라 피해자의 '\n",
      "                                            '모든 손해 중 그에 상응한 배상액을 부담케 하는 것이 손해의 '\n",
      "                                            '공평한 부담이라는 견지에서 타당하다 할 것인바, 교통사고 '\n",
      "                                            '피해자의 기왕증 등이 손해 확대에 영향을 미친 부분이 있으면 '\n",
      "                                            '입원치료기간 중의 일실수입을 산정함에 있어 이를 참작하여야 '\n",
      "                                            '한다.']}]}\n"
     ]
    }
   ],
   "source": [
    "def print_tr_sample(sample: dict) -> None:\n",
    "    tmp = copy.deepcopy(sample)\n",
    "    tmp[\"documents\"] = [tmp[\"documents\"][0], tmp[\"documents\"][-1]]\n",
    "    pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(tmp)\n",
    "\n",
    "## Print sample.\n",
    "with open(args.tr_law_data, \"r\") as f:\n",
    "    sample = json.loads(f.read())\n",
    "    \n",
    "print_tr_sample(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ba2ec96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'id': '368851881',\n",
      "        'article_original': [   '한국은행이 지난달 기준금리를 추가 인하한 영향으로 대출금리가 일제 하락했다.',\n",
      "                                '특히 기업대출금리는 1996년 관련 통계 집계 이후 가장 낮은 수준까지 하락했다.',\n",
      "                                \"한국은행이 28일 발표한 '2019년 10월 중 금융기관 가중평균금리'에 따르면 \"\n",
      "                                '신규취급액 기준 지난달 예금은행의 대출평균금리는 연 3.20%로 전월 대비 '\n",
      "                                '0.11%포인트 하락했다.',\n",
      "                                '대출금리를 기업과 가계로 나눠보면 기업대출금리 하락폭이 가팔랐다.',\n",
      "                                '10월 기업대출금리는 연 3.28%로 전월 대비 0.14%포인트 떨어졌다.',\n",
      "                                '연 3.28%는 1996년 1월 관련 통계 작성 이후 최저치다.',\n",
      "                                '대기업의 경우 기준이 되는 단기지표 금리의 하락 영향으로 10월 대출금리가 연 '\n",
      "                                '3.13%로 전월 대비 0.17%포인트 빠졌다.',\n",
      "                                '중소기업은 일부 은행의 저금리 대출 취급 등의 영향으로 연 3.39%로 '\n",
      "                                '0.11%포인트 하락했다.',\n",
      "                                '한은 관계자는 \"지난달 기준금리가 인하되면서 단기 지표에 영향을 많이 받는 '\n",
      "                                '기업대출 금리도 역대 최저 수준으로 하락했다\"고 설명했다.',\n",
      "                                '같은 기간 가계대출 금리도 하락했다.',\n",
      "                                '10월 가계대출 금리는 연 3.01%로 전월 대비 0.01%포인트 하락했다.',\n",
      "                                '같은 기간 주택담보대출 금리도 연 2.50%로 0.01%포인트 떨어졌다.',\n",
      "                                '가계대출 금리의 경우 기업대출에 비해 장기지표 영향을 많이 받아 하락폭은 작았다.',\n",
      "                                '한편 10월 중 비은행금융기관 예금금리(1년만기 정기예탁금 기준)는 모두 하락했고 '\n",
      "                                '대출금리(일반대출 기준)는 상호저축은행은 상승, 상호금융은 하락했다.'],\n",
      "        'media': '아시아경제'},\n",
      "    {   'id': '364089030',\n",
      "        'article_original': [   '올해 3분기 세계 무역기술장벽(TBT)이 역대 최대치를 기록했던 지난해 수준에 '\n",
      "                                '육박한 것으로 나타났다.',\n",
      "                                '전기·전자, 에너지, 생활용품 분야 기술장벽이 특히 높아졌다.',\n",
      "                                '개발도상국으로 확대되는 에너지효율등급 규제를 특히 조심해야 한다는 분석이 나온다.',\n",
      "                                '22일 세계무역기구(WTO)와 산업통상자원부 국가기술표준원에 따르면 올해 '\n",
      "                                '3분기까지 세계 무역기술장벽(TBT) 통보는 2329건을 기록했다.',\n",
      "                                '이는 역대 최대치 TBT 통보를 기록했던 지난해 같은 기간 2339건에 육박하는 '\n",
      "                                '수준이다.',\n",
      "                                '분야별로는 식의약품(791건), 전기·전자(276건), 생활용품(269건), '\n",
      "                                '화학세라믹(221건), 교통·안전(192건), 건설(111건), 기계(111건), '\n",
      "                                '에너지(107건) 등 순으로 나타났다.',\n",
      "                                '에너지 분야 TBT가 72건에서 107건으로 48.6% 급증했다.',\n",
      "                                '생활용품 분야도 193건에서 269건으로 39.3% 늘었고, 전기·전자 TBT '\n",
      "                                '통보도 276건으로 지난해 224건보다 23.2% 증가했다.',\n",
      "                                '반면 식의약품과 화학세라믹 분야 TBT 통보는 전년에 비해 감소한 것으로 '\n",
      "                                '나타났다.',\n",
      "                                '통보 목적별로는 인간 건강 및 안전(1521건), 품질규정(777건), 기만적 '\n",
      "                                '관행 예방 및 소비자보호(733건), 소비자 정보제공(691건), '\n",
      "                                '환경보호(435건), 국제기준 부합화(266건) 등 순으로 나타났다.',\n",
      "                                '소비자 정보제공과 소비자 보호를 위한 TBT 통보가 각각 90.3%, 53.9% '\n",
      "                                '급증했다.',\n",
      "                                '또 국제기준 부합화도 전년 142건에서 올해 266건으로 증가했다.',\n",
      "                                '세계 각국이 자신의 TBT 통보 정당성을 설명하기 위해 국제기준 부합화를 이유로 '\n",
      "                                '규제를 통보하고 있는 것으로 풀이된다.',\n",
      "                                '3분기 분야별 WTO TBT 통보건수 현황',\n",
      "                                '정부는 최근 보호무역주의가 팽배하면서 숨어있는 TBT도 확대되고 있다며 기업 '\n",
      "                                '주의를 당부했다.',\n",
      "                                'TBT는 지난해 3065건으로 역대 최대치를 기록했다.',\n",
      "                                '2015년 1977건에서 2016년 2332건, 2017년 2580건, 지난해 '\n",
      "                                '3065건으로 지속 증가 추세다.',\n",
      "                                '국표원 관계자는 \"올해도 지난해 수준으로 TBT가 비슷할 것으로 예상한다\"며 '\n",
      "                                '\"최근 사우디아라비아 등 개발도상국에서 에너지효율등급과 관련한 규제를 강화하는 '\n",
      "                                '추세여서 대응이 요구된다\"고 밝혔다.'],\n",
      "        'media': '전자신문'}]\n"
     ]
    }
   ],
   "source": [
    "def print_ts_sample(sample: list) -> None:\n",
    "    tmp = copy.deepcopy(sample)\n",
    "    tmp = [tmp[0], tmp[-1]]\n",
    "    pprint.PrettyPrinter(indent=4, sort_dicts=False).pprint(tmp)\n",
    "\n",
    "## Print sample.\n",
    "with open(args.ts_all_data, \"r\") as f:\n",
    "    sample = [json.loads(line) for line in f]\n",
    "    \n",
    "print_ts_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a7d234",
   "metadata": {},
   "source": [
    "### **DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74fc8f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        mode: str,\n",
    "        data_path: Path, \n",
    "        pt_path: Path = args.pt_path,\n",
    "        sample_submission_path: Path = args.ts_sample_submission,\n",
    "        max_cls_len: int = args.max_cls_len, ## 50\n",
    "        vocab_size: int = 512,\n",
    "        inp_pad_id: int = 0,\n",
    "        cls_pad_id: int = -1,\n",
    "        seg_pad_id: int = 0,\n",
    "        num_ext_answers: int = 3,\n",
    "        tokenizer = args.tokenizer,\n",
    "        debug: bool = False, ## referenced by Flask\n",
    "    ):\n",
    "        if not (mode in [\"tr\", \"vl\", \"ts\"]):\n",
    "            raise AssertionError(f\"Mode must be the one of 'tr', 'vl', or 'ts': not {mode}\")\n",
    "        self.mode = mode\n",
    "        \n",
    "        self.data_path = data_path\n",
    "        self.pt_path = pt_path\n",
    "        self.sample_submission_path = sample_submission_path\n",
    "        self.max_cls_len = max_cls_len\n",
    "        self.vocab_size = vocab_size ## maximum embedding length\n",
    "        self.inp_pad_id = inp_pad_id\n",
    "        self.cls_pad_id = cls_pad_id\n",
    "        self.seg_pad_id = seg_pad_id\n",
    "        self.tokenizer = tokenizer        \n",
    "        self.num_ext_answers = num_ext_answers\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "        self.dummy_tar_ext = np.array(range(self.num_ext_answers))\n",
    "        self.dummy_tar_abs = np.array([self.tokenizer.cls_token_id, self.tokenizer.sep_token_id]) ## [2, 3]\n",
    "        \n",
    "        ## Error types.\n",
    "        self.errors = {\n",
    "            \"none_in_ext_answer\": 0,\n",
    "            \"leak_of_ext_answer\": 0, ## len(tar_ext) < 3\n",
    "            \"duplicated_ext_answer\": 0, ## e.g. [0, 0, 3]\n",
    "            \"too_many_sentences\": 0,\n",
    "            \"out_of_answer_index\": 0,\n",
    "        }\n",
    "        \n",
    "        self.df = self._data_loader() ## dataframe format\n",
    "        \n",
    "        \n",
    "    def _get_documents(self, data_path: Path) -> list:\n",
    "        print(f\"Loading json data in {data_path}:\")\n",
    "        \n",
    "        documents = []\n",
    "\n",
    "        ## If training and validation phase...\n",
    "        if self.mode in [\"tr\", \"vl\"]:\n",
    "            for file_path in sorted(list(data_path.glob(\"*.json\"))):\n",
    "                print(f\"  - {file_path}...\")\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    document = json.loads(f.read()) ## list type -> not append, but extent\n",
    "                documents.extend(document[\"documents\"])\n",
    "                \n",
    "        ## If test phase...\n",
    "        elif self.mode in [\"ts\"]:\n",
    "            for file_path in sorted(list(data_path.glob(\"*.jsonl\"))):\n",
    "                print(f\"  - {file_path}...\")\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    documents.extend([json.loads(line) for line in f])\n",
    "\n",
    "        return documents\n",
    "                      \n",
    "        \n",
    "    def _data_loader(self) -> dict:\n",
    "        ## Check if the dataset already constructed or not.\n",
    "        df_name = Path(self.pt_path, f\"{self.mode}_df{'_debug' if self.debug else ''}.pt\")\n",
    "        \n",
    "        if df_name.is_file():\n",
    "            print(f\"Preprocessed dataframe is already exist: loading {df_name}...\")\n",
    "            # df = pd.DataFrame(torch.load(df_name))\n",
    "            df = torch.load(df_name)\n",
    "        \n",
    "        else:\n",
    "            print(f\"Preprocessed dataframe is not exist: constructing {df_name}...\")\n",
    "            documents = self._get_documents(self.data_path)\n",
    "            df = self._construct_dataframe(documents)\n",
    "            # torch.save(df.to_dict(), df_name)\n",
    "            torch.save(df, df_name)\n",
    "        \n",
    "        ## Print the informations.\n",
    "        print(f\"File {df_name} loaded:\")\n",
    "        print(f\"  - Shape: {df.shape}\")\n",
    "        print(f\"  - Columns: {list(df.columns)}\")\n",
    "        print(f\"  - Errors:\")\n",
    "        print(*[f\"    * {key}: {value}\" for key, value in self.errors.items()], sep=\"\\n\")\n",
    "        print(end=\"\\n\"*1)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "\n",
    "    def _construct_dataframe(self, documents: list) -> dict:\n",
    "        ## Empty dictionary.\n",
    "        data = {\"inp\": [], \"tar_ext\": [], \"tar_abs\": []}\n",
    "        # data = {\"inp\": [], \"tar_ext\": []}\n",
    "        \n",
    "        ## Training or validation phase...\n",
    "        if self.mode in [\"tr\", \"vl\"]:\n",
    "            for document in documents:\n",
    "                ## Maybe some errors can be occured in answers.\n",
    "                if None in document[\"extractive\"]:\n",
    "                    self.errors[\"none_in_ext_answer\"] += 1\n",
    "                    continue\n",
    "                    \n",
    "                if len(document[\"extractive\"]) < self.num_ext_answers:\n",
    "                    self.errors[\"leak_of_ext_answer\"] += 1\n",
    "                    continue\n",
    "                    \n",
    "                if len(np.unique(document[\"extractive\"])) < self.num_ext_answers:\n",
    "                    self.errors[\"duplicated_ext_answer\"] += 1\n",
    "                    continue\n",
    "\n",
    "                ## Elements.\n",
    "                inp = [self._clean_text(sentence[\"sentence\"]) for sentence in itertools.chain(*document[\"text\"])]\n",
    "                tar_ext = document[\"extractive\"]\n",
    "                tar_abs = document[\"abstractive\"]\n",
    "                \n",
    "                ## Check the number of sentences.\n",
    "                if len(inp) > self.max_cls_len:\n",
    "                    self.errors[\"too_many_sentences\"] += 1\n",
    "                    ## It's not a serious error, so we need to drop the documents.\n",
    "                    inp = inp[:self.max_cls_len]\n",
    "                    # continue\n",
    "                \n",
    "                ## Check if extractive answer is out-of-data or not.\n",
    "                ## Thus the element 'tar_ext' is index type, \n",
    "                ## we need to use \">= (great or equal)\", not \"> (great)\".\n",
    "                if max(tar_ext) >= self.max_cls_len:\n",
    "                    self.errors[\"out_of_answer_index\"] += 1\n",
    "                    continue\n",
    "                \n",
    "                ## Insert if flag == False.\n",
    "                data[\"inp\"].append(inp)\n",
    "                data[\"tar_ext\"].append(tar_ext)\n",
    "                data[\"tar_abs\"].append(tar_abs)\n",
    "\n",
    "                ## In development stage, we limit the maximum document size to 10 for fast experiments.\n",
    "                if self.debug and len(data[\"inp\"]) >= 1_000:\n",
    "                    break\n",
    "        \n",
    "        ## Test phase...\n",
    "        elif self.mode in [\"ts\"]:\n",
    "            ## First, we need to sort the indexes as in 'sample_submission.csv'.\n",
    "            inp_ids = np.array([int(document[\"id\"]) for document in documents])\n",
    "            tar_ids = np.array(pd.read_csv(self.sample_submission_path, index_col=False)[\"id\"])\n",
    "            \n",
    "            reallocated_idx = np.concatenate([np.where(inp_ids == i)[0] for i in tar_ids])\n",
    "            documents = np.array(documents)[reallocated_idx]\n",
    "            \n",
    "            for document in documents:                \n",
    "                ## Elements.\n",
    "                inp = [self._clean_text(sentence) for sentence in document[\"article_original\"]]\n",
    "                tar_ext = self.dummy_tar_ext ## dummy\n",
    "                tar_abs = self.dummy_tar_abs ## dummy\n",
    "                \n",
    "                ## Check the number of sentences.\n",
    "                if len(inp) > self.max_cls_len:\n",
    "                    self.errors[\"too_many_sentences\"] += 1\n",
    "                    ## We cannot drop any items in test dataset.\n",
    "                    inp = inp[:self.max_cls_len]\n",
    "                    # continue\n",
    "                \n",
    "                ## Insert.\n",
    "                data[\"inp\"].append(inp)\n",
    "                data[\"tar_ext\"].append(tar_ext)\n",
    "                data[\"tar_abs\"].append(tar_abs)\n",
    "                \n",
    "                ## In development stage, we limit the maximum document size to 10 for fast experiments.\n",
    "                # if self.debug and len(data[\"inp\"]) >= 1_000:\n",
    "                    # break\n",
    "\n",
    "        ## Convert to dataframe.\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        ## Encoding.\n",
    "        df[\"inp\"] = self._tokenize(df[\"inp\"])\n",
    "        \n",
    "        ## Generate 'cls' s.t. means the index of tokens.\n",
    "        df[\"cls\"] = df[\"inp\"].map(lambda x: np.concatenate([np.where(x == self.tokenizer.cls_token_id)[0], [len(x)]]))\n",
    "        \n",
    "        ## Generate 'seg' s.t. means segmentation embeddings which represented as [0, 0, ..., 0, 1, ..., 1, 0, 0, ...].\n",
    "        df[\"seg\"] = df[\"cls\"].map(lambda x: list(itertools.starmap(lambda x, y: [x] * y, zip(np.arange(len(np.diff(x))) % 2, np.diff(x)))))\n",
    "        df[\"seg\"] = df[\"seg\"].map(lambda x: np.array(list(itertools.chain.from_iterable(x))))\n",
    "        \n",
    "        ## Drop the last token in cls.\n",
    "        df[\"cls\"] = df[\"cls\"].map(lambda x: x[:-1])\n",
    "        \n",
    "        ## Padding.\n",
    "        self.max_inp_len = max(df[\"inp\"].map(lambda x: len(x)))\n",
    "        self.max_cls_len = max(df[\"cls\"].map(lambda x: len(x))) if self.mode in [\"ts\"] else self.max_cls_len\n",
    "        \n",
    "        df[\"inp\"] = self._pad(df[\"inp\"], self.inp_pad_id, self.max_inp_len) ## 0\n",
    "        df[\"cls\"] = self._pad(df[\"cls\"], self.cls_pad_id, self.max_cls_len) ## -1\n",
    "        df[\"seg\"] = self._pad(df[\"seg\"], self.seg_pad_id, self.max_inp_len) ## 0\n",
    "        df[\"msk\"] = df[\"inp\"].map(lambda x: ~(x == self.inp_pad_id))\n",
    "        df[\"msk_cls\"] = df[\"cls\"].map(lambda x: ~(x == self.cls_pad_id))\n",
    "        \n",
    "        ## One hot label.\n",
    "        if self.mode in [\"tr\", \"vl\"]:\n",
    "            ## Extractive.\n",
    "            df[\"tar_ext\"] = df[\"tar_ext\"].map(lambda x: self._one_hot_encoding(x))\n",
    "            ## Abstractive.\n",
    "            df[\"tar_abs\"] = self._tokenize(df[\"tar_abs\"])\n",
    "            df[\"tar_abs\"] = self._pad(df[\"inp\"], self.inp_pad_id, self.max_inp_len) ## 0\n",
    "            df[\"tar_abs_msk\"] = df[\"tar_abs\"].map(lambda x: ~(x == self.inp_pad_id))\n",
    "            \n",
    "        elif self.mode in [\"ts\"]:\n",
    "            pass\n",
    "        \n",
    "        ## Reallocate the columns' name.\n",
    "        df = df[[\"inp\", \"cls\", \"seg\", \"msk\", \"msk_cls\", \"tar_ext\", \"tar_abs\"]]\n",
    "        # df = df[[\"inp\", \"cls\", \"seg\", \"msk\", \"msk_cls\", \"tar_ext\"]]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        ## Ref. https://blog.naver.com/PostView.nhn?blogId=wideeyed&logNo=221347960543\n",
    "        \n",
    "        ## Remove email.\n",
    "        pattern = \"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+)\" \n",
    "        text = re.sub(pattern=pattern, repl=\"\", string=text)\n",
    "        \n",
    "        ## Remove URL.\n",
    "        pattern = \"(http|ftp|https)://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+\"\n",
    "        text = re.sub(pattern=pattern, repl=\"\", string=text)\n",
    "        \n",
    "        ## Stand-alone korean 자음/모음.\n",
    "        pattern = \"([ㄱ-ㅎㅏ-ㅣ]+)\"\n",
    "        text = re.sub(pattern=pattern, repl=\"\", string=text)\n",
    "        \n",
    "        ## HTML tags.\n",
    "        pattern = \"<[^>]*>\"\n",
    "        text = re.sub(pattern=pattern, repl=\"\", string=text)\n",
    "        \n",
    "        ## Specail words.\n",
    "        # pattern = \"[^\\w\\s]\"\n",
    "        # text = re.sub(pattern=pattern, repl=\"\", string=text)\n",
    "        \n",
    "        ## Strip.\n",
    "        text = text.strip()\n",
    "        \n",
    "        ## Remove double space, line feed, carrage returns.\n",
    "        text = \" \".join(text.split())\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def _tokenize(self, data: pd.Series, truncation: bool = True, add_special_tokens: bool = True) -> pd.Series:\n",
    "        ## Tokenize input and abstractive target.\n",
    "        ## Eithers must be in type of list format.\n",
    "        ##  e.g. [sent_1, sent_2, ...] -> for using 'itertools.chain.from_iterable'.\n",
    "        return data.map(lambda x: np.array(list(itertools.chain.from_iterable([self.tokenizer.encode(\n",
    "            x[i], max_length=int(self.vocab_size / len(x)), truncation=truncation, add_special_tokens=add_special_tokens,\n",
    "        ) for i in range(len(x))]))))\n",
    "        \n",
    "\n",
    "    def _pad(self, data: pd.Series, pad_id: int, max_len: int) -> pd.Series:\n",
    "        ## When len(x) == max_len, the concatenate function will be try to\n",
    "        ## concat with empty list [], and it will be forced to casting from\n",
    "        ## long (int64) to double (float32).\n",
    "        return data.map(lambda x: x if len(x) == max_len else np.concatenate([x, np.array([pad_id] * (max_len - len(x)))]))\n",
    "    \n",
    "    \n",
    "    def _one_hot_encoding(self, tar: list) -> list:\n",
    "        return np.sum(np.eye(self.max_cls_len)[np.array(tar), :], axis=0)\n",
    "    \n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    \n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        return {key: torch.from_numpy(value) for key, value in self.df.loc[idx, :].to_dict().items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b8bf2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataframe is already exist: loading data/tr_df_debug.pt...\n",
      "File data/tr_df_debug.pt loaded:\n",
      "  - Shape: (1000, 7)\n",
      "  - Columns: ['inp', 'cls', 'seg', 'msk', 'msk_cls', 'tar_ext', 'tar_abs']\n",
      "  - Errors:\n",
      "    * none_in_ext_answer: 0\n",
      "    * leak_of_ext_answer: 0\n",
      "    * duplicated_ext_answer: 0\n",
      "    * too_many_sentences: 0\n",
      "    * out_of_answer_index: 0\n",
      "\n",
      "Preprocessed dataframe is already exist: loading data/vl_df_debug.pt...\n",
      "File data/vl_df_debug.pt loaded:\n",
      "  - Shape: (1000, 7)\n",
      "  - Columns: ['inp', 'cls', 'seg', 'msk', 'msk_cls', 'tar_ext', 'tar_abs']\n",
      "  - Errors:\n",
      "    * none_in_ext_answer: 0\n",
      "    * leak_of_ext_answer: 0\n",
      "    * duplicated_ext_answer: 0\n",
      "    * too_many_sentences: 0\n",
      "    * out_of_answer_index: 0\n",
      "\n",
      "Preprocessed dataframe is already exist: loading data/ts_df_debug.pt...\n",
      "File data/ts_df_debug.pt loaded:\n",
      "  - Shape: (4161, 7)\n",
      "  - Columns: ['inp', 'cls', 'seg', 'msk', 'msk_cls', 'tar_ext', 'tar_abs']\n",
      "  - Errors:\n",
      "    * none_in_ext_answer: 0\n",
      "    * leak_of_ext_answer: 0\n",
      "    * duplicated_ext_answer: 0\n",
      "    * too_many_sentences: 0\n",
      "    * out_of_answer_index: 0\n",
      "\n",
      "CPU times: user 143 ms, sys: 36.8 ms, total: 180 ms\n",
      "Wall time: 193 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tr_ds = TrainDataset(\"tr\", data_path=args.tr_data, debug=True)\n",
    "vl_ds = TrainDataset(\"vl\", data_path=args.vl_data, debug=True)\n",
    "ts_ds = TrainDataset(\"ts\", data_path=args.ts_data, debug=True)\n",
    "\n",
    "tr_dataloader = torch.utils.data.DataLoader(tr_ds, batch_size=args.global_batch_size, num_workers=args.num_workers, shuffle=True)\n",
    "vl_dataloader = torch.utils.data.DataLoader(vl_ds, batch_size=args.global_batch_size, num_workers=args.num_workers)\n",
    "ts_dataloader = torch.utils.data.DataLoader(ts_ds, batch_size=args.global_batch_size, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f7791a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22838d32",
   "metadata": {},
   "source": [
    "## **Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f163d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class KoBERTSumExt(torch.nn.Module):\n",
    "    \n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         d_model: int = args.d_model,\n",
    "#         nhead: int = args.nhead,\n",
    "#         dropout: float = args.dropout,\n",
    "#         num_layers: int = args.num_layers,\n",
    "#         batch_first: bool = args.batch_first,\n",
    "#     ):\n",
    "#         super(KoBERTSumExt, self).__init__()\n",
    "#         ## Encoder.\n",
    "#         self.encoder = transformers.BertModel.from_pretrained(args.tokenizer_name)\n",
    "        \n",
    "#         ## Decoder.\n",
    "#         self.d_model = d_model\n",
    "#         self.nhead = nhead\n",
    "#         self.dropout = dropout\n",
    "#         self.num_layers = num_layers\n",
    "#         self.batch_first = batch_first\n",
    "        \n",
    "#         self.decoder = torch.nn.TransformerEncoder(\n",
    "#             encoder_layer=torch.nn.TransformerEncoderLayer(\n",
    "#                 d_model=self.d_model, \n",
    "#                 nhead=self.nhead, \n",
    "#                 dropout=self.dropout,\n",
    "#                 batch_first=self.batch_first,\n",
    "#             ),\n",
    "#             num_layers=self.num_layers,\n",
    "#         )\n",
    "        \n",
    "#         ## Fully connected.\n",
    "#         self.fc = torch.nn.Linear(self.d_model, 1)\n",
    "\n",
    "        \n",
    "#     @torch.cuda.amp.autocast()\n",
    "#     def forward(self, inp, cls, seg, msk, msk_cls) -> torch.tensor:\n",
    "#         ## Pretrained language model encoder.\n",
    "#         top_vec = self.encoder(\n",
    "#             input_ids=inp.long(), \n",
    "#             attention_mask=msk.float(), ## bool -> float\n",
    "#             token_type_ids=seg.long(),\n",
    "#         ).last_hidden_state\n",
    "        \n",
    "#         sents_vec = top_vec[torch.arange(top_vec.size(0)).unsqueeze(1), cls.long()]\n",
    "#         sents_vec = sents_vec * msk_cls[..., None].float() ## (batch, 50, d_model)\n",
    "        \n",
    "#         ## Transformer decoder.\n",
    "#         sent_scores = self.decoder(sents_vec)\n",
    "        \n",
    "#         ## FC layer.\n",
    "#         sent_scores = self.fc(sent_scores).squeeze(-1)\n",
    "        \n",
    "#         return sent_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5ea3b497-34a9-4e57-a349-e95812511da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoBERTSumAbs(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int = args.d_model,\n",
    "        nhead: int = args.nhead,\n",
    "        dropout: float = args.dropout,\n",
    "        num_layers: int = args.num_layers,\n",
    "        batch_first: bool = args.batch_first,\n",
    "    ):\n",
    "        super(KoBERTSumAbs, self).__init__()\n",
    "        ## Encoder.\n",
    "        self.encoder = transformers.BertModel.from_pretrained(args.tokenizer_name)\n",
    "        \n",
    "        ## Decoder.\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.dropout = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        \n",
    "        self.decoder = torch.nn.TransformerDecoder(\n",
    "            decoder_layer=torch.nn.TransformerDecoderLayer(\n",
    "                d_model=self.d_model, \n",
    "                nhead=self.nhead, \n",
    "                dropout=self.dropout,\n",
    "                batch_first=self.batch_first,\n",
    "            ),\n",
    "            num_layers=self.num_layers,\n",
    "        )\n",
    "        \n",
    "        ## Fully connected.\n",
    "        self.fc = torch.nn.Linear(self.d_model, 1)\n",
    "\n",
    "        \n",
    "    # @torch.cuda.amp.autocast()\n",
    "    def forward(self, inp, cls, seg, msk, msk_cls) -> torch.tensor:\n",
    "        ## Ref: https://github.com/nlpyang/PreSumm/blob/master/src/models/model_builder.py#L178-L244\n",
    "        \n",
    "        ## Pretrained language model encoder.\n",
    "        top_vec = self.encoder(\n",
    "            input_ids=inp.long(), \n",
    "            attention_mask=msk.float(), \n",
    "            token_type_ids=seg.long(),\n",
    "        ).last_hidden_state\n",
    "        \n",
    "        return top_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1346fc-2a84-4262-8a3f-2c7ac382eef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KoBERTSumAbs()\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b2dab57-3efb-416f-8de2-efc8d921d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in tr_dataloader:\n",
    "    aa = element\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15898132-d9a4-4982-9d91-81f7d2d44bc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9208,  0.9457,  0.8028,  ...,  1.9395,  0.8266,  0.5326],\n",
       "         [-0.6374, -0.1620,  1.1551,  ...,  0.5385,  0.2507,  0.1865],\n",
       "         [ 0.0959, -0.3112,  0.5442,  ...,  0.7617,  0.3646,  1.1576],\n",
       "         ...,\n",
       "         [-0.1439,  0.5111,  1.3784,  ...,  0.4701,  0.8532, -0.5833],\n",
       "         [-0.4965, -0.1455,  1.2110,  ...,  0.8581, -0.1202, -0.7982],\n",
       "         [-0.2997, -0.1589,  0.6536,  ...,  0.4754,  1.2939,  0.4520]],\n",
       "\n",
       "        [[-0.8884,  1.0440,  0.9928,  ...,  1.8756,  0.8537,  0.5854],\n",
       "         [-0.4987, -0.3432,  2.0792,  ..., -0.4256,  0.5372,  0.5340],\n",
       "         [-0.8129, -1.0456,  0.4533,  ...,  1.2498,  0.3255,  1.1375],\n",
       "         ...,\n",
       "         [-0.1801,  0.6060,  1.5449,  ...,  0.4092,  0.8628, -0.4671],\n",
       "         [-0.4660, -0.0193,  1.3591,  ...,  0.7593, -0.1161, -0.7434],\n",
       "         [-0.3437, -0.0353,  0.8366,  ...,  0.4233,  1.3138,  0.4615]],\n",
       "\n",
       "        [[-0.9194,  1.1448,  1.1257,  ...,  1.6773,  0.7656,  0.6172],\n",
       "         [-1.4041, -0.5533,  1.4500,  ...,  0.0036, -0.1885,  0.5503],\n",
       "         [ 0.3547,  0.0636,  0.5371,  ...,  0.2219, -0.8531,  0.8626],\n",
       "         ...,\n",
       "         [-0.2422,  0.7100,  1.7151,  ...,  0.1930,  0.7104, -0.4291],\n",
       "         [-0.4693,  0.1004,  1.4574,  ...,  0.5588, -0.2252, -0.7419],\n",
       "         [-0.4535,  0.1325,  0.9906,  ...,  0.2729,  1.2290,  0.4341]],\n",
       "\n",
       "        [[-0.7650,  1.0527,  1.0338,  ...,  1.8298,  0.8520,  0.6219],\n",
       "         [-0.6752, -0.3863,  1.3178,  ..., -0.6525,  0.6310,  0.1131],\n",
       "         [-0.1302, -0.2125,  0.7576,  ...,  0.7903,  0.6167,  0.9337],\n",
       "         ...,\n",
       "         [-0.0600,  0.6605,  1.6243,  ...,  0.4001,  0.8594, -0.4439],\n",
       "         [-0.3638,  0.0304,  1.3764,  ...,  0.7182, -0.1056, -0.7529],\n",
       "         [-0.2422,  0.0383,  0.9151,  ...,  0.3748,  1.3147,  0.4545]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = model(aa[\"inp\"], aa[\"cls\"], aa[\"seg\"], aa[\"msk\"], aa[\"msk_cls\"])\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82158b8d-8930-4d51-ba66-1167467ad43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 487, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4416b55c-5257-42d6-b361-8aa4d07aefd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(bb.size(0)).unsqueeze(1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "feaee57a-0c63-411c-a00a-ad8049762c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_vec = bb[torch.arange(bb.size(0)).unsqueeze(1), aa[\"cls\"].long()]\n",
    "sents_vec.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4d7fe3a-8c88-4c19-97b1-919bd3a8900a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0,  17,  22,  29,  70, 121, 172, 205, 256, 307,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1],\n",
       "        [  0,   5,  10,  22,  44,  95, 146, 197, 248, 299,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1],\n",
       "        [  0,  15,  19,  51,  89, 128, 167, 206, 215, 247, 268, 307, 346,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1],\n",
       "        [  0,   6,  18,  22,  41,  71, 101, 131, 156, 186, 188, 218, 248, 278,\n",
       "         308, 338, 368,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
       "          -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[\"cls\"].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea3672a-1d60-4efb-94b8-da20613026ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1669bb-8aa5-4dfd-b561-59815313ddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94874e76-9843-4b72-95e8-3b0c51cafcae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ca018-1c21-4d7e-bc6e-5370208ef585",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94bb2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(logger, model_name):\n",
    "    ## Generate model.\n",
    "    model = KoBERTSumExt()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ## Multi-gpu setting.\n",
    "    if args.n_gpus > 1:\n",
    "        logger.info(f\"{args.n_gpus} gpus available.\")\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    ## RAM to VRAM.\n",
    "    _ = model.to(args.device)\n",
    "\n",
    "    logger.info(f\"Model generated: {model_name}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d3f93",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Hyperparameters for Compile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d905e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model):\n",
    "    optimizer = torch_optimizer.RAdam(\n",
    "        model.parameters(), \n",
    "        lr=args.lr,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, steps_per_epoch: int):\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer=optimizer, \n",
    "        pct_start=args.pct_start, \n",
    "        max_lr=args.max_lr,\n",
    "        epochs=args.epochs, \n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "    )\n",
    "    return scheduler\n",
    "\n",
    "\n",
    "def get_loss_fn():\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    return loss_fn\n",
    "\n",
    "\n",
    "def get_metric_fn():\n",
    "    def hitrate(y_true, y_pred, num_ans: int = 3):\n",
    "        h = [len(list(set(ans).intersection(y_true[i]))) / num_ans for i, ans in enumerate(y_pred)]\n",
    "        score = np.mean(np.array(h))\n",
    "        return score\n",
    "    return hitrate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703cf706",
   "metadata": {},
   "source": [
    "## **Checkpoint Manager**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a33832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointManager():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        logger,\n",
    "        ckpt_dir: Path = args.ckpt_dir,\n",
    "        monitor: str = args.monitor,\n",
    "        max_to_keep: int = args.max_to_keep,\n",
    "        mode: str = \"less_good\",\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.logger = logger\n",
    "        self.ckpt_dir = ckpt_dir\n",
    "        self.monitor = monitor\n",
    "        self.max_to_keep = max_to_keep\n",
    "        self.mode = mode\n",
    "        if not (mode in [\"less_good\", \"great_good\"]):\n",
    "            raise AssertionError(f\"monitoring mode must be 'less_good' or 'great_good': not {mode}\")\n",
    "\n",
    "        self.latest_monitoring_value = np.inf if self.mode == \"less_good\" else -np.inf\n",
    "            \n",
    "        ## Directory s.t. ckpt will be stored.\n",
    "        self.save_dir = self.ckpt_dir / Path(self.model_name)\n",
    "        self.save_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        ## Ready to train log.\n",
    "        self.logger.info(f\"Checkpoint manager is now ready: save to {str(self.save_dir)}\")\n",
    "        \n",
    "    \n",
    "    def latest(self):\n",
    "        ckpt_list = sorted(list(self.ckpt_dir.glob(\"*.pt\")))\n",
    "        \n",
    "        ## If checkpoints are exist,\n",
    "        if len(ckpt_list) != 0:\n",
    "            latest_ckpt_name = ckpt_list[-1]\n",
    "            latest_ckpt = torch.load(latest_ckpt_name)\n",
    "            print(f\"Latest checkpoint {latest_ckpt_name} loaded.\")\n",
    "        \n",
    "        ## If no valid checkpoints are exist,\n",
    "        else:\n",
    "            latest_ckpt = None\n",
    "            \n",
    "        return latest_ckpt\n",
    "    \n",
    "    \n",
    "    def make_clean(self) -> None:\n",
    "        ckpt_list = sorted(list(self.save_dir.glob(\"*.pt\")))\n",
    "        \n",
    "        ## # of data must be more then 'max_to_keep'.\n",
    "        if len(ckpt_list) <= self.max_to_keep:\n",
    "            self.logger.info(f\"Noting to clean: {len(ckpt_list)} checkpoints exist.\")\n",
    "            return\n",
    "    \n",
    "        tar_ckpt_list = ckpt_list[:-self.max_to_keep]\n",
    "        free_size = sum([c.stat().st_size for c in tar_ckpt_list])\n",
    "        _ = [c.unlink() for c in tar_ckpt_list]\n",
    "        \n",
    "        self.logger.info(f\"Checkpoint folder {self.save_dir} is now clean, {free_size / (2 ** 20):.2f}MB free.\")\n",
    "    \n",
    "    \n",
    "    def save(self, epoch: int, model_state_dict: dict, optimizer_state_dict: dict, performance: dict) -> None:\n",
    "        ## Monitoring value must be located in performance dictionary.\n",
    "        monitoring_value = performance.get(self.monitor)\n",
    "        \n",
    "        ## Naming.\n",
    "        fname = self.save_dir / Path(f\"cp-{epoch:03d}-{monitoring_value:.6f}.pt\")\n",
    "        \n",
    "        ## Save it.\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model_state_dict,\n",
    "            \"optimizer_state_dict\": optimizer_state_dict,\n",
    "            self.monitor: monitoring_value,\n",
    "        }, fname)\n",
    "        \n",
    "        ## Keep memory.\n",
    "        # del model_state_dict, optimizer_state_dict\n",
    "        \n",
    "        ## Print log.\n",
    "        self.logger.info(f\"Checkpoint saved: {str(fname)}\")\n",
    "        \n",
    "        ## Update.\n",
    "        self.latest_monitoring_value = monitoring_value\n",
    "        \n",
    "        \n",
    "    def is_need_to_save(self, monitoring_value: float) -> bool:\n",
    "        if self.mode == \"less_good\":\n",
    "            answer = True if monitoring_value < self.latest_monitoring_value else False\n",
    "        elif self.mode == \"great_good\":\n",
    "            answer = True if monitoring_value > self.latest_monitoring_value else False\n",
    "            \n",
    "        return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e361f53",
   "metadata": {},
   "source": [
    "## **Trainer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74e22b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        tr_dataloader,\n",
    "        vl_dataloader,\n",
    "        global_batch_size: int = args.global_batch_size,\n",
    "        epochs: int = args.epochs,\n",
    "    ):\n",
    "        self.tr_dataloader = tr_dataloader\n",
    "        self.vl_dataloader = vl_dataloader\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        ## Define logger.\n",
    "        self.model_name = self._get_model_name()\n",
    "        \n",
    "        self.log_file = args.log_dir / Path(self.model_name, \"log.log\")\n",
    "        self.log_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        self.logger = self._get_root_logger(log_file=self.log_file)\n",
    "        \n",
    "        ## Get model and calculate training steps.\n",
    "        self.model = get_model(self.logger, self.model_name)\n",
    "        self.steps_per_epoch = len(self.tr_dataloader)\n",
    "        self.validate_steps = len(self.vl_dataloader)\n",
    "        \n",
    "        ## And the others.\n",
    "        self.device = args.device\n",
    "        self.optimizer = get_optimizer(self.model)\n",
    "        self.scheduler = get_scheduler(self.optimizer, self.steps_per_epoch)\n",
    "        self.loss_fn = get_loss_fn()\n",
    "        self.metric_fn = get_metric_fn()\n",
    "        self.ckpt_manager = CheckpointManager(model_name=self.model_name, logger=self.logger)\n",
    "        \n",
    "        self.num_ext_answers = 3\n",
    "        self.scaler = torch.cuda.amp.GradScaler()\n",
    "        \n",
    "        \n",
    "    def fit(self):\n",
    "        \n",
    "        iters = self.steps_per_epoch * self.epochs\n",
    "        self.logger.info(f\"{self.epochs} epochs ({iters} iterations) will be proceed.\")\n",
    "        \n",
    "        for epoch_index in range(self.epochs):\n",
    "            tr_result = self.train_epoch()\n",
    "            vl_result = self.validate_epoch()\n",
    "            \n",
    "            ## Print the results to stdout.\n",
    "            self.logger.info(f\"[Epoch {epoch_index + 1:02}/{self.epochs:02}]\")\n",
    "            self.logger.info(\"Train: \" + \", \".join([f\"{key}={value:.6f}\" for key, value in tr_result.items()]))\n",
    "            self.logger.info(\"Valid: \" + \", \".join([f\"{key}={value:.6f}\" for key, value in vl_result.items()]))\n",
    "            \n",
    "            ## Record performance.\n",
    "            performance = {}\n",
    "            performance.update({f\"tr_{key}\": value for key, value in tr_result.items()})\n",
    "            performance.update({f\"vl_{key}\": value for key, value in vl_result.items()})\n",
    "            \n",
    "            ## Save.\n",
    "            monitoring_value = performance.get(self.ckpt_manager.monitor)\n",
    "            if monitoring_value == None:\n",
    "                raise ValueError(f\"Performances must have the element '{self.ckpt_manager.monitor}': {performance.keys()}\")\n",
    "            \n",
    "            if self.ckpt_manager.is_need_to_save(monitoring_value):\n",
    "                self.ckpt_manager.save(\n",
    "                    epoch=epoch_index,\n",
    "                    model_state_dict=self.model.state_dict(),\n",
    "                    optimizer_state_dict=self.optimizer.state_dict(),\n",
    "                    performance=performance,\n",
    "                )\n",
    "                \n",
    "        ## Make clean after all epochs.\n",
    "        self.ckpt_manager.make_clean()\n",
    "            \n",
    "            \n",
    "    def predict(self):\n",
    "        pass\n",
    "                \n",
    "    \n",
    "    def train_epoch(self) -> dict:\n",
    "        self.model.train()\n",
    "        self.tr_total_loss = 0\n",
    "        \n",
    "        tot_y_true = []\n",
    "        tot_y_pred = []\n",
    "        \n",
    "        tqdm_dataloader = tqdm(self.tr_dataloader, ascii=True, ncols=95)\n",
    "        \n",
    "        ## Batch iteration.\n",
    "        for batch_index, data in enumerate(tqdm_dataloader):\n",
    "            ## Initialize optimizer.\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            ## Unpack.\n",
    "            inp = data[\"inp\"].to(self.device)\n",
    "            cls = data[\"cls\"].to(self.device)\n",
    "            seg = data[\"seg\"].to(self.device)\n",
    "            msk = data[\"msk\"].to(self.device)\n",
    "            msk_cls = data[\"msk_cls\"].to(self.device)\n",
    "            \n",
    "            tar_ext = data[\"tar_ext\"].to(self.device)\n",
    "            # tar_abs = data[\"tar_abs\"].to(self.device)\n",
    "            \n",
    "            ## Forward.\n",
    "            with torch.cuda.amp.autocast():\n",
    "                sent_score = self.model(inp, cls, seg, msk, msk_cls)\n",
    "                loss = (self.loss_fn(sent_score, tar_ext.float()) * msk_cls.float()).sum() / sent_score.size(0)\n",
    "                                \n",
    "            ## Backward.\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            \n",
    "            scale = self.scaler.get_scale()\n",
    "            self.scaler.update()\n",
    "            \n",
    "            ## No update scheduler when errors occured in mixed precision policy.\n",
    "            if scale == self.scaler.get_scale():\n",
    "                self.scheduler.step()\n",
    "\n",
    "            ## Keep loss.\n",
    "            self.tr_total_loss += loss\n",
    "                \n",
    "            ## Inference.\n",
    "            cur_y_true = torch.where(tar_ext == 1)[1].reshape(-1, self.num_ext_answers).tolist()\n",
    "            cur_y_pred = torch.topk(torch.sigmoid(sent_score) * msk_cls.float(), self.num_ext_answers, axis=1).indices.tolist()\n",
    "            \n",
    "            tot_y_true.extend(cur_y_true)\n",
    "            tot_y_pred.extend(cur_y_pred)\n",
    "            \n",
    "            ## Show the results.\n",
    "            tqdm_dataloader.set_postfix(self._get_assets(batch_index, self.tr_total_loss, tot_y_true, tot_y_pred))\n",
    "            \n",
    "        ## Total loss and accuracy in one epoch.\n",
    "        return self._get_assets(batch_index, self.tr_total_loss, tot_y_true, tot_y_pred, as_str=False)\n",
    "    \n",
    "    \n",
    "    def validate_epoch(self) -> dict:\n",
    "        self.model.eval()\n",
    "        self.vl_total_loss = 0\n",
    "        \n",
    "        tot_y_true = []\n",
    "        tot_y_pred = []\n",
    "                \n",
    "        ## Batch iteration with no gradient.\n",
    "        with torch.no_grad():\n",
    "            for batch_index, data in enumerate(self.vl_dataloader):\n",
    "                ## Initialize optimizer.\n",
    "                self.optimizer.zero_grad()\n",
    "\n",
    "                ## Unpack.\n",
    "                inp = data[\"inp\"].to(self.device)\n",
    "                cls = data[\"cls\"].to(self.device)\n",
    "                seg = data[\"seg\"].to(self.device)\n",
    "                msk = data[\"msk\"].to(self.device)\n",
    "                msk_cls = data[\"msk_cls\"].to(self.device)\n",
    "\n",
    "                tar_ext = data[\"tar_ext\"].to(self.device)\n",
    "                # tar_abs = data[\"tar_abs\"].to(self.device)\n",
    "\n",
    "                ## Forward.\n",
    "                sent_score = self.model(inp, cls, seg, msk, msk_cls)\n",
    "                loss = (self.loss_fn(sent_score, tar_ext.float()) * msk_cls.float()).sum() / sent_score.size(0)\n",
    "\n",
    "                ## Keep loss.\n",
    "                self.vl_total_loss += loss\n",
    "\n",
    "                ## Inference.\n",
    "                cur_y_true = torch.where(tar_ext == 1)[1].reshape(-1, self.num_ext_answers).tolist()\n",
    "                cur_y_pred = torch.topk(torch.sigmoid(sent_score) * msk_cls.float(), self.num_ext_answers, axis=1).indices.tolist()\n",
    "\n",
    "                tot_y_true.extend(cur_y_true)\n",
    "                tot_y_pred.extend(cur_y_pred)\n",
    "\n",
    "        ## Total loss and accuracy in one epoch.\n",
    "        return self._get_assets(batch_index, self.vl_total_loss, tot_y_true, tot_y_pred, as_str=False)    \n",
    "    \n",
    "    \n",
    "    def _get_assets(self, batch_index: int, total_loss: float, y_true: list, y_pred: list, as_str: bool = True) -> dict:\n",
    "        ## We define to show loss and accuracy.\n",
    "        assets = {\n",
    "            \"loss\": total_loss / (batch_index + 1),\n",
    "            \"hitrate\": self.metric_fn(y_true=y_true, y_pred=y_pred),\n",
    "        }\n",
    "        ## When using tqdm dataloader, we need to convert float to string.\n",
    "        if as_str:\n",
    "            assets = {key: f\"{value:.6f}\" for key, value in assets.items()}\n",
    "        \n",
    "        return assets\n",
    "    \n",
    "    \n",
    "    def _get_model_name(self):\n",
    "        KST = datetime.timezone(datetime.timedelta(hours=9))\n",
    "        model_name = datetime.datetime.now(tz=KST).strftime(\"%Y%m%d-%H%M%S\")\n",
    "        return model_name\n",
    "    \n",
    "    \n",
    "    def _get_root_logger(self, log_level: int = logging.INFO, log_file: Path = None):\n",
    "        ## Get logger.\n",
    "        logger = logging.getLogger(self.model_name)\n",
    "        \n",
    "        # If the logger has been initialized, just return it\n",
    "        if logger.hasHandlers():\n",
    "            return logger\n",
    "\n",
    "        format_str = \"%(asctime)s %(levelname)s: %(message)s\"\n",
    "        logging.basicConfig(format=format_str, level=log_level)\n",
    "\n",
    "        if log_file != None:\n",
    "            file_handler = logging.FileHandler(log_file, 'w')\n",
    "            file_handler.setFormatter(logging.Formatter(format_str))\n",
    "            file_handler.setLevel(log_level)\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143c794",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed69ee3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 11:09:08,437 INFO: Model generated: 20211107-110904\n",
      "2021-11-07 11:09:08,438 INFO: Checkpoint manager is now ready: save to ckpt/20211107-110904\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(tr_dataloader, vl_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3afc3503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-07 11:09:08,441 INFO: 10 epochs (2500 iterations) will be proceed.\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.22it/s, loss=7.130616, hitrate=0.293000]\n",
      "2021-11-07 11:09:52,626 INFO: [Epoch 01/10]\n",
      "2021-11-07 11:09:52,627 INFO: Train: loss=7.130616, hitrate=0.293000\n",
      "2021-11-07 11:09:52,628 INFO: Valid: loss=8.957650, hitrate=0.285667\n",
      "2021-11-07 11:09:53,838 INFO: Checkpoint saved: ckpt/20211107-110904/cp-000-8.957650.pt\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.29it/s, loss=6.410256, hitrate=0.431667]\n",
      "2021-11-07 11:10:37,690 INFO: [Epoch 02/10]\n",
      "2021-11-07 11:10:37,691 INFO: Train: loss=6.410256, hitrate=0.431667\n",
      "2021-11-07 11:10:37,691 INFO: Valid: loss=8.916684, hitrate=0.342000\n",
      "2021-11-07 11:10:39,018 INFO: Checkpoint saved: ckpt/20211107-110904/cp-001-8.916684.pt\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=6.116788, hitrate=0.476333]\n",
      "2021-11-07 11:11:23,032 INFO: [Epoch 03/10]\n",
      "2021-11-07 11:11:23,032 INFO: Train: loss=6.116788, hitrate=0.476333\n",
      "2021-11-07 11:11:23,033 INFO: Valid: loss=10.013276, hitrate=0.366333\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.24it/s, loss=5.991924, hitrate=0.489667]\n",
      "2021-11-07 11:12:07,227 INFO: [Epoch 04/10]\n",
      "2021-11-07 11:12:07,227 INFO: Train: loss=5.991924, hitrate=0.489667\n",
      "2021-11-07 11:12:07,227 INFO: Valid: loss=10.290118, hitrate=0.360333\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=5.897597, hitrate=0.497000]\n",
      "2021-11-07 11:12:51,397 INFO: [Epoch 05/10]\n",
      "2021-11-07 11:12:51,398 INFO: Train: loss=5.897597, hitrate=0.497000\n",
      "2021-11-07 11:12:51,399 INFO: Valid: loss=10.249619, hitrate=0.362333\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=5.793322, hitrate=0.504000]\n",
      "2021-11-07 11:13:35,588 INFO: [Epoch 06/10]\n",
      "2021-11-07 11:13:35,589 INFO: Train: loss=5.793322, hitrate=0.504000\n",
      "2021-11-07 11:13:35,590 INFO: Valid: loss=9.897532, hitrate=0.361000\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=5.704447, hitrate=0.524667]\n",
      "2021-11-07 11:14:19,754 INFO: [Epoch 07/10]\n",
      "2021-11-07 11:14:19,755 INFO: Train: loss=5.704447, hitrate=0.524667\n",
      "2021-11-07 11:14:19,755 INFO: Valid: loss=10.413079, hitrate=0.361333\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=5.623479, hitrate=0.530000]\n",
      "2021-11-07 11:15:03,906 INFO: [Epoch 08/10]\n",
      "2021-11-07 11:15:03,906 INFO: Train: loss=5.623479, hitrate=0.530000\n",
      "2021-11-07 11:15:03,907 INFO: Valid: loss=10.885634, hitrate=0.362667\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.24it/s, loss=5.560882, hitrate=0.527000]\n",
      "2021-11-07 11:15:48,112 INFO: [Epoch 09/10]\n",
      "2021-11-07 11:15:48,113 INFO: Train: loss=5.560882, hitrate=0.527000\n",
      "2021-11-07 11:15:48,114 INFO: Valid: loss=10.953753, hitrate=0.358333\n",
      "100%|#######################| 250/250 [00:34<00:00,  7.25it/s, loss=5.504683, hitrate=0.531667]\n",
      "2021-11-07 11:16:32,250 INFO: [Epoch 10/10]\n",
      "2021-11-07 11:16:32,251 INFO: Train: loss=5.504683, hitrate=0.531667\n",
      "2021-11-07 11:16:32,251 INFO: Valid: loss=11.078527, hitrate=0.359000\n",
      "2021-11-07 11:16:32,384 INFO: Noting to clean: 2 checkpoints exist.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318586b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bbf42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06b675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a010a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2237a6d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568a270a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38-torch",
   "language": "python",
   "name": "py38-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "scrolled": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
